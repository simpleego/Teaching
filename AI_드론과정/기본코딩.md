# AI 드론과정 기본 코딩

### **1. 펌웨어 개발 (Firmware) ⚙️**

드론의 가장 낮은 레벨에서 하드웨어를 직접 제어하는 파트입니다. 비행 제어 컴퓨터(FC)나 보조 컴퓨터(Companion Computer)에서 실행됩니다.

  * **주요 작업:**
      * **센서 데이터 수집:** 자이로, 가속도, GPS 등 비행 센서의 데이터를 읽어옵니다.
      * **모터 제어:** ESC(변속기)에 PWM 신호를 보내 모터의 회전 속도를 제어합니다.
      * **통신 프로토콜 구현:** 지상 관제 시스템(GCS)이나 외부 장치와 MAVLink, UART, I2C 등의 프로토콜로 통신합니다.
  * **주요 기술:** `C/C++`, `Python`, `Arduino`, `PX4`, `ArduPilot`
  * **코드 예시 (Arduino/C++ 스타일):**
    ```cpp
    #include <Servo.h> // ESC 제어를 위한 라이브러리

    Servo esc_motor; // ESC 객체 생성

    void setup() {
      // ESC를 핀 9에 연결하고 신호 범위 초기화
      esc_motor.attach(9, 1000, 2000); 
    }

    void loop() {
      // 모터를 특정 속도로 설정 (예: 1500 마이크로초 신호)
      int speed = 1500; 
      esc_motor.writeMicroseconds(speed);
      delay(100);
    }
    ```

-----

### **2. 드론 영상 실시간 수집 및 전송 📡**

드론의 카메라에서 영상을 받아와 지상국(서버)으로 실시간 스트리밍하는 파트입니다.

  * **주요 작업:**
      * 카메라 하드웨어에서 영상 프레임을 연속적으로 캡처합니다.
      * 캡처한 프레임을 H.264와 같은 표준 코덱으로 압축(인코딩)합니다.
      * 압축된 영상 데이터를 RTSP, WebRTC, UDP 등의 프로토콜을 사용해 네트워크로 전송합니다.
  * **주요 기술:** `Python`, `OpenCV`, `GStreamer`, `FFmpeg`
  * **코드 예시 (Python + GStreamer):**
    ```python
    # GStreamer 파이프라인을 사용해 카메라 영상을 RTSP 서버로 스트리밍하는 명령어
    # 실제로는 os.system() 이나 subprocess 모듈로 실행

    pipeline = (
        "gst-launch-1.0 nvarguscamerasrc ! "
        "'video/x-raw(memory:NVMM), width=1280, height=720, framerate=30/1' ! "
        "nvvidconv ! nvv4l2h264enc ! h264parse ! rtph264pay name=pay0 pt=96 ! "
        "udpsink host=<서버_IP_주소> port=5000"
    )

    # 이 파이프라인은 NVIDIA Jetson 장치에서 카메라 영상을 받아와
    # H.264로 인코딩한 후 UDP를 통해 서버로 전송합니다.
    ```

-----

### **3. 영상 데이터 전처리 (Preprocessing) 🎨**

서버에서 수신한 영상 데이터를 인공지능 모델이 분석하기 좋은 형태로 가공하는 단계입니다.

  * **주요 작업:**
      * 수신된 영상 스트림을 디코딩하여 개별 프레임으로 분리합니다.
      * AI 모델의 입력 크기에 맞게 이미지 해상도를 조절(resize)합니다.
      * 이미지 색상 체계를 변환(예: BGR → RGB)하고, 픽셀 값을 정규화(Normalization)합니다.
  * **주요 기술:** `Python`, `OpenCV`, `NumPy`, `Pillow`
  * **코드 예시 (Python + OpenCV):**
    ```python
    import cv2
    import numpy as np

    def preprocess_frame(frame):
      # 1. 모델 입력 크기(예: 640x640)로 리사이즈
      input_size = (640, 640)
      resized_frame = cv2.resize(frame, input_size)

      # 2. BGR 색상을 RGB로 변환 (PyTorch 모델 기준)
      rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)

      # 3. 픽셀 값을 0~1 사이로 정규화
      normalized_frame = rgb_frame.astype(np.float32) / 255.0

      # 4. 모델 입력에 맞게 차원 변경 (H, W, C) -> (C, H, W)
      transposed_frame = np.transpose(normalized_frame, (2, 0, 1))

      return transposed_frame
    ```

-----

### **4. 물체감지 인공지능 모듈 개발 (AI Detection) 🤖**

전처리된 영상 프레임에서 특정 물체를 찾아내는 AI 모델을 구현하고 실행하는 핵심 파트입니다.

  * **주요 작업:**
      * YOLO, SSD 등 사전 학습된(pre-trained) 물체 감지 모델을 로드합니다.
      * 전처리된 프레임을 모델에 입력하여 추론(Inference)을 실행합니다.
      * 모델의 출력(물체의 경계 상자, 클래스, 신뢰도 점수)을 해석합니다.
  * **주요 기술:** `Python`, `PyTorch`, `TensorFlow`, `ONNX`, `OpenCV DNN`
  * **코드 예시 (Python + PyTorch/YOLOv5):**
    ```python
    import torch

    # YOLOv5 모델 로드 (사전에 훈련된 가중치 파일 yolov5s.pt 사용)
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

    def detect_objects(frame):
      # 전처리된 프레임을 모델에 입력
      results = model(frame)

      # 결과 파싱 (감지된 객체들의 정보 추출)
      # 예: results.pandas().xyxy[0]
      detections = results.pandas().xyxy[0] 
      
      # 감지된 객체 정보(좌표, 이름, 신뢰도) 반환
      return detections
    ```

-----

### **5. 드론 영상 송수신 애플리케이션 개발 🖥️**

모든 기능을 통합하여 사용자가 드론 영상을 보고, 물체 감지 결과를 확인할 수 있는 최종 프로그램을 만듭니다.

  * **주요 작업:**

      * **백엔드:** 드론으로부터 영상 스트림을 수신하고, AI 모델로 분석한 뒤, 결과를 프론트엔드로 전달하는 서버를 구축합니다.
      * **프론트엔드:** 백엔드에서 받은 영상과 객체 데이터를 화면에 실시간으로 표시합니다. 영상 위에 경계 상자(Bounding Box)를 그립니다.

  * **주요 기술:**

      * **백엔드:** `Python`, `FastAPI`, `Flask`, `WebSocket`
      * **프론트엔드:** `HTML/CSS`, `JavaScript`, `React`, `Vue.js`

  * **코드 예시 (Python/FastAPI + JavaScript):**

    **백엔드 (main.py):**

    ```python
    from fastapi import FastAPI
    from fastapi.websockets import WebSocket

    app = FastAPI()

    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
      await websocket.accept()
      while True:
        # 1. 드론으로부터 영상 프레임 수신 (가정)
        frame = receive_drone_frame() 
        # 2. AI 모델로 객체 감지
        results = detect_objects(frame)
        # 3. 결과 데이터를 JSON 형태로 프론트엔드에 전송
        await websocket.send_json(results.to_dict('records'))
    ```

    **프론트엔드 (index.html + script.js):**

    ```javascript
    // WebSocket 서버에 연결
    const ws = new WebSocket("ws://localhost:8000/ws");

    ws.onmessage = function(event) {
      const detections = JSON.parse(event.data);
      const canvas = document.getElementById('myCanvas');
      const ctx = canvas.getContext('2d');
      
      // 이전 프레임 지우기
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      // 수신한 좌표로 사각형(Bounding Box) 그리기
      detections.forEach(d => {
        ctx.strokeRect(d.xmin, d.ymin, d.xmax - d.xmin, d.ymax - d.ymin);
        ctx.fillText(d.name, d.xmin, d.ymin - 5);
      });
    };
    ```
