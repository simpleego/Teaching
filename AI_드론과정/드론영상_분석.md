# **드론–스트리밍–분석

---

## 전체 그림 한 번에 보기

| 단계 | 핵심 역할 | 주요 기술 |
|------|-----------|-----------|
| 1. 드론 영상 → RTSP 변환 | 드론/카메라 영상을 표준 스트림으로 만들기 | RTSP/RTMP, MediaMTX, FFmpeg |
| 2. DJI 드론 실시간 분석 구조 | DJI 생태계에 맞는 수신·중계 구조 설계 | DJI Cloud API, RTMP/RTSP/WebRTC |
| 3. YOLO 분석 파이프라인 | 들어온 스트림을 AI로 분석 | OpenCV, FFmpeg 파이프, YOLOv8/11 |

---

## 1. “드론 영상 → RTSP 변환 시스템” 설계

여기서 목표는 단순해:

> **“드론이 어떤 방식으로 영상을 보내든, 최종적으로는 RTSP로 정리해서 분석 시스템이 먹기 좋게 만드는 것”**

### 1-1. 기본 구조

1. **드론 → 지상 수신 장치**
   - 드론이 보내는 영상:  
     - 전용 프로토콜 (DJI, Autel 등)  
     - RTMP  
     - USB/HDMI 출력  
2. **지상 수신 장치 → RTSP 변환 서버**
   - MediaMTX(구 rtsp-simple-server) 같은 RTSP/RTMP 허브 사용  
   - FFmpeg로 입력을 받아 H.264/H.265로 인코딩 후 RTSP로 재송출  
3. **RTSP → 분석 시스템**
   - `rtsp://server-ip:8554/drone` 같은 형태로 YOLO/분석 서버에서 구독

### 1-2. 예시 아키텍처

- 드론 → RTMP 송출  
- MediaMTX가 RTMP를 받아 RTSP로 재송출  
- 분석 서버는 RTSP만 바라봄

```text
[Drone Camera]
      │ (RTMP)
      ▼
[MediaMTX / Streaming Server]
      │ (RTSP)
      ▼
[YOLO 분석 서버 / 관제 시스템]
```

이 구조의 장점:

- 분석 시스템은 **항상 RTSP만 처리**하면 됨  
- 드론 종류가 바뀌어도 “앞단 변환”만 손보면 됨  
- CCTV, 드론, IP 카메라를 한 시스템에 섞어 쓰기 쉬움

---

## 2. DJI 드론 영상 실시간 분석 구조

DJI는 RTSP를 바로 안 주는 경우가 많아서, **DJI 생태계에 맞는 구조**를 따로 잡아야 해.

### 2-1. DJI 쪽에서 제공하는 스트리밍 방식

DJI Cloud API / Demo Web 기준으로 지원하는 프로토콜들:

- RTMP  
- RTSP  
- GB28181  
- WebRTC  
- Agora 기반 스트리밍

즉, DJI 드론 → DJI Cloud API → 여러 프로토콜로 송출하는 구조를 이미 갖고 있어.

### 2-2. 실시간 분석 구조 예시

1. **DJI 드론 → DJI Cloud API / RC / 앱**
   - DJI SDK / Cloud API로 영상 수신
2. **Cloud API / 중계 서버 → RTMP/RTSP 송출**
   - RTMP/RTSP/GB28181/WebRTC 중 선택 가능
3. **MediaMTX 또는 자체 스트리밍 서버**
   - RTMP 입력 → RTSP/웹 대시보드로 재분배
4. **YOLO 분석 서버**
   - RTSP를 받아 실시간 분석

실제 구현 예로, DJI 드론 + YOLO + MediaMTX + 웹 UI를 통합한 프로젝트도 있어:

- DJI 드론 → RTMP → MediaMTX  
- MediaMTX → RTSP/RTMP 제공  
- Python YOLO 분석기 + 웹 인터페이스로 실시간 시각화

---

## 3. 드론 영상 YOLO 분석 파이프라인

이제 마지막 단계—**RTSP로 들어온 드론 영상을 YOLO로 분석하는 흐름**이야.

### 3-1. 파이프라인 개념

1. **입력 계층**
   - RTSP/RTMP/파일/가상 RTSP 등  
   - OpenCV `VideoCapture(rtsp_url)` 또는 FFmpeg 파이프 방식
2. **디코딩 계층**
   - H.264/H.265 → raw frame (BGR)  
   - OpenCV 또는 FFmpeg로 처리
3. **전처리 계층**
   - 리사이즈, 정규화, FPS 조절
4. **YOLO 추론 계층**
   - YOLOv8/YOLOv11 (ultralytics)  
   - GPU 사용 시 실시간 처리 가능
5. **후처리 계층**
   - NMS, 클래스 필터링, ROI 침입 감지, 카운팅
6. **출력 계층**
   - 화면 표시, 웹소켓 전송, 로그 저장, 알림

### 3-2. RTSP + YOLO 실시간 처리 패턴

- 단순 OpenCV 방식:

```python
cap = cv2.VideoCapture(rtsp_url)
ret, frame = cap.read()
# → YOLO 모델에 frame 전달
```

- RTSP를 FFmpeg로 받아 파이프로 넘기는 고급 방식도 많이 쓰임  
  - 이유: 일부 카메라/드론은 OpenCV `VideoCapture`로 RTSP를 제대로 못 읽는 경우가 있어서,  
    FFmpeg로 먼저 받아 raw frame을 Python으로 넘기는 구조가 더 안정적임.

### 3-3. 실제 프로젝트 예시

- GitHub의 “DJI Human Detection System” 구조:
  - MediaMTX로 RTSP/RTMP 허브 구성  
  - Python YOLO 분석기에서 RTSP 스트림을 받아 실시간 사람 탐지  
  - 웹 UI로 시각화 + Socket.IO로 실시간 전송

이건 jong이 만들고 있는 시스템과 거의 같은 방향이야—  
**드론/카메라 → 스트리밍 서버 → YOLO 분석기 → 관제 UI**.

---

## 정리하면

1. **드론 영상 → RTSP 변환 시스템**
   - 어떤 드론이든, 최종적으로 RTSP로 정리하는 허브(MediaMTX + FFmpeg) 설계
2. **DJI 드론 실시간 분석 구조**
   - DJI Cloud API / RTMP / RTSP / WebRTC를 활용해 중계 서버를 두고, 분석 서버는 표준 스트림만 받도록 설계
3. **YOLO 분석 파이프라인**
   - RTSP 입력 → 디코딩 → YOLO 추론 → 후처리(침입, 카운팅, 로그) → 시각화/알림까지 이어지는 일관된 흐름 구성

---

이제 여기서부터는  
- “실제 아키텍처 다이어그램”  
- “jong 환경에 맞는 구체적인 기술 스택 선택 (MediaMTX vs 직접 FFmpeg 파이프 등)”  
- “YOLO 분석 서버를 어떻게 배포할지 (단일 PC, Docker, 클라우드)”  
같은 걸 같이 정리해볼 수 있어.

어디까지 현실 시스템으로 가져가보고 싶어?
