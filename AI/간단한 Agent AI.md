# ê°„ë‹¨í•œ Agent AI ì†ŒìŠ¤ ì½”ë“œ

--- 

```python

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# pip install langchain langchain-openai langgraph faiss-cpu tiktoken

from typing import TypedDict, List, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA
from langgraph.graph import StateGraph

# 1. ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
class RAGState(TypedDict):
    question: str
    answer: Optional[str]
    sources: Optional[List[str]]

# 2. ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°í™”
loader = TextLoader("docs/sample.txt")  # ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ê²½ë¡œ
documents = loader.load()
embedding = OpenAIEmbeddings(openai_api_key="api key"
vectorstore = FAISS.from_documents(documents, embedding)

# 3. RAG ì²´ì¸ êµ¬ì„±
retriever = vectorstore.as_retriever()
rag_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4", openai_api_key="api key"),
    retriever=retriever,
    return_source_documents=True
)

# 4. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def retrieve_node(state: RAGState) -> RAGState:
    result = rag_chain({"query": state["question"]})
    return {
        "question": state["question"],
        "answer": result["result"],
        "sources": result["source_documents"]
    }

# 5. ê·¸ë˜í”„ êµ¬ì„±
graph = StateGraph(RAGState)
graph.add_node("retrieve", retrieve_node)
graph.set_entry_point("retrieve")
graph.set_finish_point("retrieve")
app = graph.compile()

# 6. ì‹¤í–‰
state = {"question": "ì‚¼ì„±ì „ìì˜ ìµœê·¼ ì‹¤ì ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"}
result = app.invoke(state)

# 7. ê²°ê³¼ ì¶œë ¥
print("ğŸ’¬ ì§ˆë¬¸:", result["question"])
print("ğŸ“Œ ë‹µë³€:", result["answer"])
print("ğŸ“š ì¶œì²˜ ë¬¸ì„œ ìˆ˜:", len(result["sources"]))

```

